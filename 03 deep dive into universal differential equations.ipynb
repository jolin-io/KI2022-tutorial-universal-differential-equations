{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jolin-io/KI2022-tutorial-universal-differential-equations/main?filepath=03%20deep%20dive%20into%20universal%20differential%20equations.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.jolin.io\" target=\"_blank\" rel=\"noreferrer noopener\">\n",
    "<img src=\"https://www.jolin.io/assets/Jolin/Jolin-Banner-Website-v1.1-darkmode.webp\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep dive into Universal Differential Equations in <img height=\"60px\" style='height:60px;display:inline;' alt=\"Julia\" src=\"https://julialang.org/assets/infra/logo.svg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DifferentialEquations, SciMLSensitivity, DiffEqFlux\n",
    "import Symbolics, ModelingToolkit, DataDrivenDiffEq\n",
    "import Optimization, OptimizationOptimisers, OptimizationOptimJL\n",
    "import Lux, ComponentArrays\n",
    "import Plots, Random, Statistics, StatsBase, DelimitedFiles\n",
    "\n",
    "using CommonSolve: solve\n",
    "\n",
    "rng = Random.default_rng()\n",
    "Random.seed!(rng, 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outline of this extensive deep dive:\n",
    "1. Scientific Machine Learning with UDEs\n",
    "    1. Differential Equations\n",
    "    2. DiffEq within Machine Learning\n",
    "    3. Machine Learning within DiffEq\n",
    "    4. Machine Learning within DiffEq - alternative perspective\n",
    "    5. More UDEs\n",
    "2. Symbolic Regression with DataDrivenDiffEq\n",
    "    1. Symbolic regression without UDE\n",
    "    2. Symbolic regression with UDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Machine Learning with UDEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term Universal Differential Equations was introduced in the paper [Universal Differential Equations for Scientific\n",
    "Machine Learning by Rackauckas et. al. 2021](https://arxiv.org/pdf/2001.04385.pdf)\n",
    "\n",
    "**UDE is about using machine learning as part of differential equation problems.** As such it is one way of combining scientific model-based approaches with machine learning techniques, which is often named scientific machine learning. \n",
    "\n",
    "Another combination of machine learning and differential equations are for example physics-informed neural networks (PINN). These are not the topic of today, but have a look at [NeuralPDE.jl](https://github.com/SciML/NeuralPDE.jl) if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here an overview over the scientific machine learning ecosystem as described in the UDE paper:\n",
    "![](./assets/overview_sciml_ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a huge ecosystem. For today we focuse mostly on the last layer of implementing Differential Equations which depend on Neural Networks directly.\n",
    "\n",
    "The following code is a compilation and update from [official UDE paper example](https://github.com/ChrisRackauckas/universal_differential_equations/blob/master/LotkaVolterra)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start with some real data\n",
    "\n",
    "The data has been taken from https://jmahaffy.sdsu.edu/courses/f00/math122/labs/labj/q3v1.htm\n",
    "(Originally published in E. P. Odum (1953), Fundamentals of Ecology, Philadelphia, W. B. Saunders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hudson_bay_data = DelimitedFiles.readdlm(\"assets/hudson_bay_data.dat\", '\\t', Float32, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize time to start at 0\n",
    "t = hudson_bay_data[:, 1] .- hudson_bay_data[1, 1]\n",
    "tspan = (t[begin], t[end])\n",
    "\n",
    "# Measurements of prey and predator\n",
    "X = Matrix(transpose(hudson_bay_data[:, 2:3]))\n",
    "# Normalize the data; since the data domain is strictly positive\n",
    "# we just need to divide by the maximum\n",
    "xscale = maximum(X, dims =2)\n",
    "X .= 1f0 ./ xscale .* X\n",
    "\n",
    "# Plot the data\n",
    "Plots.scatter(t, X', xlabel = \"t\", ylabel = \"x(t), y(t)\")\n",
    "Plots.plot!(t, X', xlabel = \"t\", ylabel = \"x(t), y(t)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DifferentialEquations.jl\n",
    "\n",
    "We can model this data using the [Lotka-Volterra equations](https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations): Population of rabbits and foxes\n",
    "\n",
    "<center>\n",
    "\n",
    "rabbits: $ x^\\prime = \\alpha x - \\beta x y $\n",
    "\n",
    "</center>\n",
    "\n",
    "the rate of change of the prey's population is given by its own growth rate ($\\alpha$) minus the rate at which it is preyed upon ($\\beta$).\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "foxes: $ y^\\prime = \\gamma x y - \\delta y $\n",
    "\n",
    "</center>\n",
    "\n",
    "the rate of change of the predator's population depends upon the rate at which it consumes prey ($\\gamma$), minus its intrinsic death rate ($\\delta$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function lotka_volterra(du, u, p, t)\n",
    "    x, y = u\n",
    "    Î±, Î², Î´, Î³ = p\n",
    "    du[1] = dx = Î±*x - Î²*x*y\n",
    "    du[2] = dy = -Î´*y + Î³*x*y\n",
    "end\n",
    "u0 = X[:, 1]\n",
    "tspan = (0.0, 20.0)\n",
    "p = [0.6, 0.8, 0.6, 0.8]\n",
    "ode_prob = DifferentialEquations.ODEProblem(lotka_volterra, u0, tspan, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.plot(solve(ode_prob, saveat=t))\n",
    "Plots.scatter!(t, X', xlabel = \"t\", ylabel = \"x(t), y(t)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's fit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DiffEq within Machine Learning\n",
    "\n",
    "This just means we learn the DiffEq parameters via gradient-based Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(parameters, ode_prob=ode_prob, t=t)\n",
    "    solve(ode_prob, saveat = t, p = parameters)\n",
    "end\n",
    "function loss_function(parameters, data)\n",
    "    pred = Array(predict(parameters))\n",
    "    return sum(abs2, pred .- data)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_initial = ode_prob.p\n",
    "Plots.plot(predict(ps_initial))\n",
    "Plots.scatter!(t, X', xlabel = \"t\", ylabel = \"x(t), y(t)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(ps_initial, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = Float64[]\n",
    "function callback(p, l)\n",
    "    push!(losses, l)\n",
    "    if length(losses) % 50 == 0\n",
    "        Plots.plot(losses, show = :inline, yscale = :log10,\n",
    "            label = \"loss\", xlabel = \"#epochs\", ylabel=\"loss (log10 scale)\")\n",
    "    end\n",
    "    return false  # return bool `halt`\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "minimizer = ps_initial\n",
    "\n",
    "opt_function = Optimization.OptimizationFunction(\n",
    "    (ps, data) -> loss_function(ps, data),\n",
    "    Optimization.AutoZygote(),\n",
    ")\n",
    "\n",
    "for (optimizer, maxiters) = [\n",
    "        (OptimizationOptimisers.Adam(0.1), 300),\n",
    "        (OptimizationOptimisers.Adam(0.01), 500),\n",
    "    ]\n",
    "    opt_prob = Optimization.OptimizationProblem(opt_function, minimizer, X)\n",
    "    opt_sol = solve(opt_prob, optimizer, callback = callback, maxiters = maxiters)\n",
    "    minimizer = opt_sol.minimizer\n",
    "end\n",
    "\n",
    "ps_trained = minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots.plot(predict(ps_trained))\n",
    "Plots.scatter!(t, transpose(X), xlabel = \"t\", ylabel = \"x(t), y(t)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ try different initial parameter, make the problem harder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ experiment with the optimizers [Adam](https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.Adam) and try different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning within DiffEq\n",
    "\n",
    "We can use machine learning within Differential Equations to leave parts of our model as black-box which are going to be learned.\n",
    "\n",
    "For simplicity we generate data from some fully known model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_ideal = ps_trained\n",
    "ode_prob_ideal = DifferentialEquations.ODEProblem(lotka_volterra, u0, tspan, ps_ideal)\n",
    "ode_sol_ideal = solve(ode_prob_ideal, saveat = 0.1)\n",
    "\n",
    "# Ideal data\n",
    "X_ideal = Array(ode_sol_ideal)\n",
    "t = ode_sol_ideal.t\n",
    "\n",
    "noise_magnitude = 5e-2\n",
    "X = X_ideal\n",
    "X = X .+ (noise_magnitude*Statistics.mean(X, dims=2)) .* randn(eltype(X), size(X))\n",
    "\n",
    "Plots.plot(ode_sol_ideal)\n",
    "Plots.plot!(t, X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The machine learning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian RBF as activation\n",
    "rbf(x) = exp.(-(x.^2))\n",
    "\n",
    "# Define the network 2->5->5->5->2\n",
    "model_lux = Lux.Chain(\n",
    "    Lux.Dense(2,5,rbf),\n",
    "    Lux.Dense(5,5, rbf),\n",
    "    Lux.Dense(5,5, tanh),\n",
    "    Lux.Dense(5,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_lux, st_lux = Lux.setup(rng, model_lux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing ml into the differential equations\n",
    "\n",
    "We model the linear terms explicitly (like we are sure that linear terms are apt for our scenario), but leave the interaction-terms to our neural black-box.\n",
    "\n",
    "Let's see whether we can reconstruct the interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hybrid model\n",
    "function ude_dynamics!(du,u, p, t)\n",
    "    u_pred, _st_lux = model_lux(u, p.ps_lux, st_lux) # Network prediction\n",
    "    # We assume a linear birth rate for the prey\n",
    "    du[1] = p.ps_ode[1]*u[1] + u_pred[1]\n",
    "    # We assume a linear decay rate for the predator\n",
    "    du[2] = -p.ps_ode[2]*u[2] + u_pred[2]\n",
    "end\n",
    "\n",
    "# Get the initial parameters, first two are linear birth/decay of prey and predator\n",
    "ps_initial = ComponentArrays.ComponentVector((\n",
    "    ps_ode = rand(rng, Float32, 2),\n",
    "    ps_lux = ps_lux,\n",
    "))\n",
    "u0 = X[:, 1]\n",
    "ode_prob_nn = DifferentialEquations.ODEProblem(ude_dynamics!, u0, tspan, ps_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(parameters, t=t, ode_prob=ode_prob_nn)\n",
    "    solve(\n",
    "        ode_prob,\n",
    "        p = parameters,\n",
    "        saveat = t,\n",
    "        # sensealg = SciMLSensitivity.ForwardDiffSensitivity()\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ plot our initial guess for the ode solution (given the initial parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training is a bit more elaborate. We first use a special training loss provided by `DiffEqFlux`. It is called `muliple_shoot` which essentially devides the training data into pieces and learns on the single pieces instead of learning everything at once.\n",
    "\n",
    "For more details on `multiple_shoot` see the [DiffEqFlux.jl documentation](https://diffeqflux.sciml.ai/stable/examples/multiple_shooting/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function shooting_loss(parameters, X=X, t=t, ode_prob=ode_prob_nn,\n",
    "        group_size=5, continuity_term=200.0f0)\n",
    "    \n",
    "    loss_compare(data, pred) = sum(abs2, data - pred)\n",
    "    \n",
    "    loss, pred = DiffEqFlux.multiple_shoot(\n",
    "        parameters, X, t, ode_prob, loss_compare, DifferentialEquations.Tsit5(), group_size;\n",
    "        continuity_term = continuity_term)\n",
    "    loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a standard loss over the full data. The loss comes with an extra penalty which forces parameters to be small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss(parameters, X=X)\n",
    "    _X_pred = predict(parameters)\n",
    "    _X_pred.retcode == :Success || return Inf\n",
    "    X_pred = Array(_X_pred)\n",
    "    loss_diff = sum(abs2, X - X_pred) / size(X, 2)\n",
    "    \n",
    "    loss_penalty = sum(abs2, parameters.ps_lux) / length(parameters.ps_lux)\n",
    "    factor_penalty = convert(eltype(parameters), 1e-4)\n",
    "    loss_diff + factor_penalty * loss_penalty \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ run both losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Container to track the losses\n",
    "losses = Float32[]\n",
    "\n",
    "# Callback to show the loss during training\n",
    "callback(parameters, args...) = begin\n",
    "    l = loss(parameters) # Equivalent L2 loss\n",
    "    push!(losses, l)\n",
    "    if length(losses) % 50 == 0\n",
    "        Plots.plot(losses, show = :inline, yscale = :log10,\n",
    "            label = \"loss\", xlabel = \"#epochs\", ylabel=\"loss (log10 scale)\")\n",
    "    end\n",
    "    return false  # return bool `halt`\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we train with different optimizers and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = Float32[]\n",
    "minimizer = ps_initial\n",
    "\n",
    "for (opt_alg, maxiters, loss_func) = [\n",
    "        (OptimizationOptimisers.Adam(0.01), 200, shooting_loss)\n",
    "        (OptimizationOptimisers.Adam(0.01), 100, loss)\n",
    "    ]\n",
    "    opt_func = Optimization.OptimizationFunction((ps, _) -> loss_func(ps), Optimization.AutoZygote())   \n",
    "    opt_prob = Optimization.OptimizationProblem(opt_func, minimizer) \n",
    "    opt_sol = solve(opt_prob, opt_alg, maxiters = maxiters, callback = callback)\n",
    "    minimizer = opt_sol.minimizer\n",
    "end\n",
    "ps_trained = minimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = Array(predict(ps_trained))\n",
    "\n",
    "# Trained on noisy data vs real solution\n",
    "p1 = Plots.scatter(t, X[1,:], label=\"Data\", alpha=0.2, xlabel = \"t\", ylabel = \"u1(t)\")\n",
    "Plots.plot!(t, X_ideal[1,:], label=\"ideal\")\n",
    "Plots.plot!(t, X_pred[1,:], label=\"UDE Approximation\")\n",
    "\n",
    "p2 = Plots.scatter(t, X[2,:], label=\"Data\", alpha=0.2, xlabel = \"t\", ylabel = \"u2(t)\")\n",
    "Plots.plot!(t, X_ideal[2,:], label=\"ideal\")\n",
    "Plots.plot!(t, X_pred[2,:], label = \"UDE Approximation\")\n",
    "\n",
    "Plots.plot(p1, p2, layout=(2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh! We need to improve.\n",
    "\n",
    "ðŸ‘‰ adapt the training procedure(the number of iterations, the [Adam](https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.Adam) config, ...) to make our model fit the data at least reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@show ps_trained.ps_ode\n",
    "@show ode_prob_ideal.p[[1, 3]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ fix one of the linear factors to the true value and see whether we can now infer the other correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating the future\n",
    "\n",
    "ðŸ‘‰ now that the training looks good, let's check whether the model is stable on the long run\n",
    "\n",
    "simulate our `nn_ode_prob` for some time into the future (hint: you may want to change `tspan`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Machine Learning within DiffEq - alternative perspective \n",
    "\n",
    "The famous paper **Neural Ordinary Differential Equations (Chen et al. 2019)** introduced the following intuition for Neural Ordinary Differential Equations.\n",
    "\n",
    "Residual Neural Network (discrete difference layers)\n",
    "$$h_{t+1} = h_t + f(h_t, \\theta_t)$$\n",
    "\n",
    "Neural Ordinary Differential Equations\n",
    "$$\\frac{dh(t)}{dt} = f(h(t), t, \\theta)$$\n",
    "\n",
    "![](https://www.jolin.io/assets/examples/NeuralODE-Comparing-ResNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More UDEs\n",
    "\n",
    "One key aspect of Julia's scientific machine learning stack is the immense features it provides.\n",
    "\n",
    "Just a short summary from the UDE paper.\n",
    "\n",
    "![UDE features](./assets/ude_overview_features.png)\n",
    "\n",
    "and here the benchmarks\n",
    "![UDE benchmarks](./assets/ude_benchmarks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic regression\n",
    "\n",
    "Symbolic regressions is the discipline of fitting mathematical formular to given data. We use DataDrivenDiffEq.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, generate the basis functions, multivariate polynomials up to deg 5 and sine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Symbolics.@variables u[1:2]\n",
    "b = DataDrivenDiffEq.polynomial_basis(u, 5)\n",
    "basis = DataDrivenDiffEq.Basis(b, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic regression without UDE\n",
    "\n",
    "for all symbolic regression we use the same solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the thresholds which should be used in the search process\n",
    "Î» = Float32.(exp10.(-7:0.1:5))\n",
    "# Create an optimizer for the SINDy problem\n",
    "opt = DataDrivenDiffEq.STLSQ(Î»)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "symbolic regression from true derivate data works perfectly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DX = Array(ode_sol_ideal(t, Val{1}))\n",
    "ddd_prob_fullideal = DataDrivenDiffEq.DataDrivenProblem(X, t = t, DX = DX)\n",
    "\n",
    "ddd_sol_fullideal = solve(ddd_prob_fullideal, basis, opt,\n",
    "    maxiter = 10_000, progress = true, denoise = true, normalize = true)\n",
    "\n",
    "println(ddd_sol_fullideal)\n",
    "println(DataDrivenDiffEq.result(ddd_sol_fullideal))\n",
    "println()\n",
    "println(DataDrivenDiffEq.parameter_map(ddd_sol_fullideal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Symbolic regression from approx derivatives via collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the problem using a gaussian kernel for collocation\n",
    "ddd_prob_fullreal = DataDrivenDiffEq.ContinuousDataDrivenProblem(X, t, DataDrivenDiffEq.GaussianKernel())\n",
    "ddd_sol_fullreal = solve(ddd_prob_fullreal, basis, opt,\n",
    "    maxiter = 10_000, progress = true, denoise = true, normalize = true)\n",
    "\n",
    "println(ddd_sol_fullreal)\n",
    "println(DataDrivenDiffEq.result(ddd_sol_fullreal))\n",
    "println()\n",
    "println(DataDrivenDiffEq.parameter_map(ddd_sol_fullreal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we compare the prediction of the derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic regression with UDE\n",
    "\n",
    "We want to apply symbolic regression to the neural network part.\n",
    "\n",
    "Importantly, the neural net only captured the **interactions** between predators and prey.\n",
    "The **linear parts** were already given (structurely), and fit separately -  they don't matter here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we used `DataDrivenDiffEq` to approximate a derivative.\n",
    "We can actually use the same package to appxorimate direct functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network guess\n",
    "Y_nn, _st_lux = model_lux(X_pred, ps_trained.ps_lux, st_lux)\n",
    "\n",
    "ddd_prob_nn = DataDrivenDiffEq.DataDrivenProblem(X_pred, Y=Y_nn)\n",
    "ddd_sol_nn = solve(ddd_prob_nn, basis, opt, maxiter = 10_000, progress = true, normalize = false, denoise = true)\n",
    "\n",
    "println(ddd_sol_nn)\n",
    "println(DataDrivenDiffEq.result(ddd_sol_nn))\n",
    "println()\n",
    "println(DataDrivenDiffEq.parameter_map(ddd_sol_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks kind of okay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check, that the symbolic regression `X~Y` really works, we can apply it to the ideal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ideal = [\n",
    "    -ps_ideal[2] * (X_ideal[1,:] .* X_ideal[2,:])'\n",
    "    ps_ideal[4] * (X_ideal[1,:] .* X_ideal[2,:])'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ apply `DataDrivenProblem` to ideal case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That was the deep-dive into Universal Differential Equations in julia - Thank you for participating ðŸ™‚\n",
    "\n",
    "I've prepared a **bonus topic** about combining differential equations with bayesian inference, i.e. probabilistic parameter and error estimation: [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jolin-io/KI2022-tutorial-universal-differential-equations/main?filepath=04%20introduction%20to%20bayesian%20differential%20equations.ipynb)\n",
    "\n",
    "If you have question, suggestions, or you are just interested in Julia, contact me:\n",
    "- Stephan Sahm stephan.sahm@jolin.io\n",
    "\n",
    "### Further Material\n",
    "\n",
    "- [Blog Post DiffEqFlux.jl](https://julialang.org/blog/2019/01/fluxdiffeq/)\n",
    "- [Documentation DiffEqFlux.jl](https://diffeqflux.sciml.ai/stable/)\n",
    "- [Paper Neural Ordinary Differential Equations (Chen et al. 2019)](https://arxiv.org/abs/1806.07366)\n",
    "- [Paper Universal Differential Equations for SciML (Rackauckas et al. 2020)](https://arxiv.org/abs/2001.04385)\n",
    "- [Documentation DataDrivenDiffEq.jl](https://datadriven.sciml.ai/stable), [linear ODE example](https://datadriven.sciml.ai/stable/examples/2_linear_continuous_system/), [nonlinear ODE example](https://datadriven.sciml.ai/stable/examples/4_nonlinear_continuous_system/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.jolin.io\" target=\"_blank\" rel=\"noreferrer noopener\">\n",
    "<img src=\"https://www.jolin.io/assets/Jolin/Jolin-Banner-Website-v1.1-darkmode.webp\">\n",
    "</a>\n",
    "\n",
    "#### Supported by [Jolin.io](https://www.jolin.io)\n",
    "\n",
    "Jolin.io is an IT-consultancy for high-performance computing and data science\n",
    "\n",
    "We are there to help you, if you want to\n",
    "- try out Julia at your company, or\n",
    "- transition Matlab, Fortran, R, Python, etc. to Julia\n",
    "- or speed up your existing code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia",
   "language": "julia",
   "name": "julia"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "520e995520d0f28b9f1e7cacfd9ba1493aa60b57e5f0cc1543205df7dd9220a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jolin-io/KI2022-tutorial-universal-differential-equations/main?filepath=03%20deep%20dive%20into%20universal%20differential%20equations.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.jolin.io\" target=\"_blank\" rel=\"noreferrer noopener\">\n",
    "<img src=\"https://www.jolin.io/assets/Jolin/Jolin-Banner-Website-v1.1-darkmode.webp\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep dive into Universal Differential Equations in <img height=\"60px\" style='height:60px;display:inline;' alt=\"Julia\" src=\"https://julialang.org/assets/infra/logo.svg\">\n",
    "\n",
    "Outline of this extensive deep dive:\n",
    "1. Scientific Machine Learning with UDEs\n",
    "    1. Differential Equations\n",
    "    2. DiffEq within Machine Learning\n",
    "    3. Machine Learning within DiffEq\n",
    "    4. Machine Learning within DiffEq - alternative perspective\n",
    "    5. More UDEs\n",
    "2. Symbolic Regression with DataDrivenDiffEq\n",
    "    1. Symbolic regression without UDE\n",
    "    2. Symbolic regression with UDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Machine Learning with UDEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term Universal Differential Equations was introduced in the paper [Universal Differential Equations for Scientific\n",
    "Machine Learning by Rackauckas et. al. 2021](https://arxiv.org/pdf/2001.04385.pdf)\n",
    "\n",
    "**UDE is about using machine learning as part of differential equation problems.** As such it is one way of combining scientific model-based approaches with machine learning techniques, which is often named scientific machine learning. \n",
    "\n",
    "Another combination of machine learning and differential equations are for example physics-informed neural networks (PINN). These are not the topic of today, but have a look at [NeuralPDE.jl](https://github.com/SciML/NeuralPDE.jl) if you are interested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here an overview over the scientific machine learning ecosystem as described in the UDE paper:\n",
    "![](./assets/overview_sciml_ecosystem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a huge ecosystem. For today we focuse mostly on the last layer of implementing Differential Equations which depend on Neural Networks directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import DifferentialEquations, DiffEqSensitivity, DiffEqFlux\n",
    "import Symbolics, ModelingToolkit, DataDrivenDiffEq\n",
    "import Optimization, OptimizationOptimisers, OptimizationOptimJL\n",
    "import Lux, ComponentArrays\n",
    "import Plots, Random, Statistics, StatsBase, DelimitedFiles\n",
    "\n",
    "using CommonSolve: solve\n",
    "\n",
    "rng = Random.default_rng()\n",
    "Random.seed!(rng, 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DifferentialEquations.jl\n",
    "\n",
    "Example [Lotka-Volterra equations](https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations): Population of rabbits and foxes\n",
    "\n",
    "<center>\n",
    "\n",
    "rabbits: $ x^\\prime = \\alpha x - \\beta x y $\n",
    "\n",
    "</center>\n",
    "\n",
    "the rate of change of the prey's population is given by its own growth rate ($\\alpha$) minus the rate at which it is preyed upon ($\\beta$).\n",
    "\n",
    "\n",
    "<center>\n",
    "\n",
    "foxes: $ y^\\prime = \\gamma x y - \\delta y $\n",
    "\n",
    "</center>\n",
    "\n",
    "the rate of change of the predator's population depends upon the rate at which it consumes prey ($\\gamma$), minus its intrinsic death rate ($\\delta$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function lotka_volterra(du, u, p, t)\n",
    "    x, y = u\n",
    "    Î±, Î², Î´, Î³ = p\n",
    "    du[1] = dx = Î±*x - Î²*x*y\n",
    "    du[2] = dy = -Î´*y + Î³*x*y\n",
    "end\n",
    "u0 = [1.0, 1.0]\n",
    "tspan = (0.0, 10.0)\n",
    "p = [1.5, 1.0, 3.0, 1.0]\n",
    "ode_prob = DifferentialEquations.ODEProblem(lotka_volterra, u0, tspan, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ode_sol = solve(ode_prob, saveat=0.1)\n",
    "Plots.plot(ode_sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DiffEq within Machine Learning\n",
    "\n",
    "This just means we learn the DiffEq parameters via gradient-based Optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(parameters, ode_prob=ode_prob, t=ode_sol.t)\n",
    "    solve(ode_prob, saveat = t, p = parameters)\n",
    "end\n",
    "function loss_function(parameters, data)\n",
    "    pred = Array(predict(parameters))[1,:]\n",
    "    return sum(abs2, pred .- data)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_initial = Random.rand!(similar(ode_prob.p))\n",
    "Plots.plot(predict(ps_initial))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_initial = ode_prob.p\n",
    "data = 1.0\n",
    "loss_function(ps_initial, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "losses = Float64[]\n",
    "function callback(p, l)\n",
    "    push!(losses, l)\n",
    "    if length(losses) % 50 == 0\n",
    "        Plots.plot(losses, show = :inline, yscale = :log10,\n",
    "            label = \"loss\", xlabel = \"#epochs\", ylabel=\"loss (log10 scale)\")\n",
    "    end\n",
    "    return false  # return bool `halt`\n",
    "end\n",
    "\n",
    "ps_trained = let data = data\n",
    "    minimizer = ps_initial\n",
    "    opt_function = Optimization.OptimizationFunction(\n",
    "        (ps, data) -> loss_function(ps, data),\n",
    "        Optimization.AutoZygote(),\n",
    "    )\n",
    "    for (optimizer, maxiters) = [\n",
    "            (OptimizationOptimisers.Adam(0.1), 300),\n",
    "            (OptimizationOptimisers.Adam(0.01), 500),\n",
    "        ]\n",
    "        opt_prob = Optimization.OptimizationProblem(opt_function, minimizer, data)\n",
    "        opt_sol = solve(opt_prob, optimizer,\n",
    "            callback = callback, maxiters = maxiters)\n",
    "        minimizer = opt_sol.minimizer\n",
    "    end\n",
    "    minimizer\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ experiment with the optimizers [Adam](https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.Adam) and try different configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ plot the initial prediction before and after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning within DiffEq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has been taken from https://jmahaffy.sdsu.edu/courses/f00/math122/labs/labj/q3v1.htm\n",
    "(Originally published in E. P. Odum (1953), Fundamentals of Ecology, Philadelphia, W. B. Saunders)\n",
    "\n",
    "The code was updated from the slightly out-dated [official UDE paper example](https://github.com/ChrisRackauckas/universal_differential_equations/blob/master/LotkaVolterra/hudson_bay.jl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hudson_bay_data = DelimitedFiles.readdlm(\"assets/hudson_bay_data.dat\", '\\t', Float32, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize time to start at 0\n",
    "t = hudson_bay_data[:, 1] .- hudson_bay_data[1, 1]\n",
    "tspan = (t[begin], t[end])\n",
    "\n",
    "# Measurements of prey and predator\n",
    "X = Matrix(transpose(hudson_bay_data[:, 2:3]))\n",
    "# Normalize the data; since the data domain is strictly positive\n",
    "# we just need to divide by the maximum\n",
    "xscale = maximum(X, dims =2)\n",
    "X .= 1f0 ./ xscale .* X\n",
    "\n",
    "# Plot the data\n",
    "Plots.scatter(t, transpose(X), xlabel = \"t\", ylabel = \"x(t), y(t)\")\n",
    "Plots.plot!(t, transpose(X), xlabel = \"t\", ylabel = \"x(t), y(t)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The machine learning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian RBF as activation\n",
    "rbf(x) = exp.(-(x.^2))\n",
    "\n",
    "# Define the network 2->5->5->5->2\n",
    "model_lux = Lux.Chain(\n",
    "    Lux.Dense(2,5,rbf),\n",
    "    Lux.Dense(5,5, rbf),\n",
    "    Lux.Dense(5,5, tanh),\n",
    "    Lux.Dense(5,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_lux, st_lux = Lux.setup(rng, model_lux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bringing ml into the differential equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hybrid model\n",
    "function ude_dynamics!(du,u, p, t)\n",
    "    u_pred, _st_lux = model_lux(u, p.ps_lux, st_lux) # Network prediction\n",
    "    # We assume a linear birth rate for the prey\n",
    "    du[1] = p.ps_ode[1]*u[1] + u_pred[1]\n",
    "    # We assume a linear decay rate for the predator\n",
    "    du[2] = -p.ps_ode[2]*u[2] + u_pred[2]\n",
    "end\n",
    "\n",
    "# Get the initial parameters, first two are linear birth/decay of prey and predator\n",
    "p_initial = ComponentArrays.ComponentVector((\n",
    "    ps_ode = rand(rng, Float32, 2),\n",
    "    ps_lux = ps_lux,\n",
    "))\n",
    "ode_prob_nn = DifferentialEquations.ODEProblem(ude_dynamics!, X[:, 1], tspan, p_initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training is a bit more elaborate. We first use a special training loss provided by `DiffEqFlux`. It is called `muliple_shoot` which essentially devides the training data into pieces and learns on the single pieces instead of learning everything at once.\n",
    "\n",
    "For more details on `multiple_shoot` see the [DiffEqFlux.jl documentation](https://diffeqflux.sciml.ai/stable/examples/multiple_shooting/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for Multiple Shooting\n",
    "group_size = 5\n",
    "continuity_term = 200.0f0\n",
    "\n",
    "function shooting_loss(parameters)\n",
    "    loss_compare(data, pred) = sum(abs2, data - pred)\n",
    "    loss, pred = DiffEqFlux.multiple_shoot(\n",
    "        parameters, X, t, ode_prob_nn, loss_compare, DifferentialEquations.Vern7(), group_size;\n",
    "        continuity_term = continuity_term)\n",
    "    loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a standard predictor and loss.\n",
    "\n",
    "The loss comes with an extra penalty which forces parameters to be small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict(parameters, t = t)\n",
    "    solve(\n",
    "        ode_prob_nn,\n",
    "        DifferentialEquations.Vern7(),\n",
    "        p = parameters,\n",
    "        saveat = t,\n",
    "        abstol = 1e-6, reltol = 1e-6,\n",
    "        sensealg = DiffEqSensitivity.ForwardDiffSensitivity()\n",
    "    )\n",
    "end\n",
    "\n",
    "function loss(parameters)\n",
    "    X_pred = Array(predict(parameters))\n",
    "    loss_diff = sum(abs2, X - X_pred) / size(X, 2)\n",
    "    loss_penalty = sum(abs2, parameters.ps_lux) / length(parameters.ps_lux)\n",
    "    factor_penalty = convert(eltype(parameters), 1e-3)\n",
    "    loss_diff + factor_penalty * loss_penalty \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ run both losses and visualize the predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Container to track the losses\n",
    "losses = Float32[]\n",
    "\n",
    "# Callback to show the loss during training\n",
    "callback(parameters, args...) = begin\n",
    "    l = loss(parameters) # Equivalent L2 loss\n",
    "    push!(losses, l)\n",
    "    if length(losses) % 50 == 0\n",
    "        Plots.plot(losses, show = :inline, yscale = :log10,\n",
    "            label = \"loss\", xlabel = \"#epochs\", ylabel=\"loss (log10 scale)\")\n",
    "    end\n",
    "    return false  # return bool `halt`\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we train twice, first with faster learning rate, second with slower learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "minimizer = p_initial\n",
    "\n",
    "for (opt_alg, maxiters, loss_func) = [\n",
    "        (OptimizationOptimisers.Adam(0.1), 100, shooting_loss),\n",
    "        (OptimizationOptimisers.Adam(0.01), 100, loss),\n",
    "    ]\n",
    "    opt_func = Optimization.OptimizationFunction((ps, _) -> loss_func(ps), Optimization.AutoZygote())   \n",
    "    opt_prob = Optimization.OptimizationProblem(opt_func, minimizer) \n",
    "    opt_sol = solve(opt_prob, opt_alg, maxiters = maxiters, callback = callback)\n",
    "    minimizer = opt_sol.minimizer\n",
    "    println(\"Training loss after $(length(losses)) iterations: $(losses[end])\")\n",
    "end\n",
    "p_trained = minimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did it work out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpolate the solution\n",
    "tsample = t[1]:0.5:t[end]\n",
    "X_pred = Array(predict(p_trained, tsample))\n",
    "# Trained on noisy data vs real solution\n",
    "Plots.scatter(t, X', label = [\"Measurements\" nothing], xlabel = \"t\", ylabel = \"x(t), y(t)\")\n",
    "Plots.plot!(tsample, X_pred', label = [\"UDE Approximation\" nothing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh! We need to improve.\n",
    "\n",
    "ðŸ‘‰ adapt the training procedure(the number of iterations, the [Adam](https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.Adam) config, ...) to make our model fit the data at least reasonable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating the future\n",
    "\n",
    "ðŸ‘‰ now that the training looks good, let's check whether the model is stable on the long run\n",
    "\n",
    "simulate our `ode_prob_nn` for some time into the future (hint: you may want to change `tspan`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Machine Learning within DiffEq - alternative perspective \n",
    "\n",
    "The famous paper **Neural Ordinary Differential Equations (Chen et al. 2019)** introduced the following intuition for Neural Ordinary Differential Equations.\n",
    "\n",
    "Residual Neural Network (discrete difference layers)\n",
    "$$h_{t+1} = h_t + f(h_t, \\theta_t)$$\n",
    "\n",
    "Neural Ordinary Differential Equations\n",
    "$$\\frac{dh(t)}{dt} = f(h(t), t, \\theta)$$\n",
    "\n",
    "![](https://www.jolin.io/assets/examples/NeuralODE-Comparing-ResNet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More UDEs\n",
    "\n",
    "One key aspect of Julia's scientific machine learning stack is the immense features it provides.\n",
    "\n",
    "Just a short summary from the UDE paper.\n",
    "\n",
    "![UDE features](./assets/ude_overview_features.png)\n",
    "\n",
    "and here the benchmarks\n",
    "![UDE benchmarks](./assets/ude_benchmarks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic regression\n",
    "\n",
    "Symbolic regressions is the discipline of fitting mathematical formular to given data. We use DataDrivenDiffEq.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, generate the basis functions, multivariate polynomials up to deg 5 and sine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Symbolics.@variables u[1:2]\n",
    "b = DataDrivenDiffEq.polynomial_basis(u, 5)\n",
    "basis = DataDrivenDiffEq.Basis(b, u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic regression without UDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct Identification via SINDy + Collocation (estimates derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the problem using a gaussian kernel for collocation\n",
    "full_problem = DataDrivenDiffEq.ContinuousDataDrivenProblem(X, t, DataDrivenDiffEq.GaussianKernel())\n",
    "# Create the thresholds which should be used in the search process\n",
    "Î» = Float32.(exp10.(-7:0.1:5))\n",
    "# Create an optimizer for the SINDy problem\n",
    "opt = DataDrivenDiffEq.STLSQ(Î»)\n",
    "\n",
    "full_res = solve(full_problem, basis, opt,\n",
    "    maxiter = 10_000, progress = true, denoise = true, normalize = true)\n",
    "\n",
    "println(full_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(DataDrivenDiffEq.result(full_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(DataDrivenDiffEq.parameter_map(full_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pred = full_res(full_problem.X, full_res.parameters, full_problem.t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Plots.plot(full_problem.t, full_problem.DX[1,:], label=\"collocation du1\")\n",
    "Plots.plot!(full_problem.t, full_pred[1,:], label=\"symbolic regression du1\")\n",
    "\n",
    "p2 = Plots.plot(full_problem.t, full_problem.DX[2,:], label=\"collocation du2\")\n",
    "Plots.plot!(full_problem.t, full_pred[2,:], label=\"symbolic regression du2\")\n",
    "\n",
    "Plots.plot(p1, p2, layout=(2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic regression with UDE\n",
    "\n",
    "We want to apply symbolic regression to the neural network part.\n",
    "\n",
    "Importantly, the neural net only captured the **interactions** between predators and prey.\n",
    "The **linear parts** were already given (structurely), and fit separately -  they don't matter here.\n",
    "\n",
    "First, let's look at what we actually learned in our neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard Lotka Volterra \n",
    "p_ideal = ode_prob.p\n",
    "Y_ideal = [\n",
    "    -p_ideal[2] * (X_pred[1,:] .* X_pred[2,:])'\n",
    "    p_ideal[4] * (X_pred[1,:] .* X_pred[2,:])'\n",
    "]\n",
    "\n",
    "# prediction of global data driven approach, minus linear learned terms\n",
    "full_pred2 = full_res(X_pred, full_res.parameters, tsample)\n",
    "full_problem_DX_nn_only = full_pred2 - [1, -1] .* p_trained.ps_ode .* X_pred\n",
    "\n",
    "# Neural network guess\n",
    "Y_pred, _st_lux = model_lux(X_pred, p_trained.ps_lux, st_lux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Plots.plot(tsample, Y_ideal[1,:], label = \"Ideal Lotka Volterra\")\n",
    "Plots.plot!(tsample, full_problem_DX_nn_only[1,:], label = \"symbolic regression without ude\")\n",
    "Plots.plot!(tsample, Y_pred[1,:], label = \"UDE\")\n",
    "\n",
    "p2 = Plots.plot(tsample, Y_ideal[2,:], label = \"Ideal Lotka Volterra\")\n",
    "Plots.plot!(tsample, full_problem_DX_nn_only[2,:], label = \"symbolic regression without ude\")\n",
    "Plots.plot!(tsample, Y_pred[2,:], label = \"UDE\")\n",
    "\n",
    "Plots.plot(p1, p2, layout=(2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this looks reasonable, let's start the symbolic regression.\n",
    "\n",
    "We can now directly specify a function relationship (and don't need to deal with derivatives here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_problem = DataDrivenDiffEq.DataDrivenProblem(X_pred, Y=Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_res = solve(nn_problem, basis, opt, maxiter = 10_000, progress = true, normalize = false, denoise = true)\n",
    "println(nn_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(DataDrivenDiffEq.result(nn_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(DataDrivenDiffEq.parameter_map(nn_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_pred = nn_res(X_pred, nn_res.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Plots.plot(nn_pred[1,:], label=\"symbolic regression\")\n",
    "Plots.plot!(Y_pred[1,:], label=\"NeuralODE prediction\")\n",
    "\n",
    "p2 = Plots.plot(nn_pred[2,:], label=\"symbolic regression\")\n",
    "Plots.plot!(Y_pred[2,:], label=\"NeuralODE prediction\")\n",
    "\n",
    "Plots.plot(p1, p2, layout=(2,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ apply `DataDrivenProblem` to ideal case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That was the deep-dive into Universal Differential Equations in julia - Thank you for participating ðŸ™‚\n",
    "\n",
    "I've prepared a **bonus topic** about combining differential equations with bayesian inference, i.e. probabilistic parameter and error estimation: [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jolin-io/KI2022-tutorial-universal-differential-equations/main?filepath=04%20introduction%20to%20bayesian%20differential%20equations.ipynb)\n",
    "\n",
    "If you have question, suggestions, or you are just interested in Julia, contact me:\n",
    "- Stephan Sahm stephan.sahm@jolin.io\n",
    "\n",
    "### Further Material\n",
    "\n",
    "- [Blog Post DiffEqFlux.jl](https://julialang.org/blog/2019/01/fluxdiffeq/)\n",
    "- [Documentation DiffEqFlux.jl](https://diffeqflux.sciml.ai/stable/)\n",
    "- [Paper Neural Ordinary Differential Equations (Chen et al. 2019)](https://arxiv.org/abs/1806.07366)\n",
    "- [Paper Universal Differential Equations for SciML (Rackauckas et al. 2020)](https://arxiv.org/abs/2001.04385)\n",
    "- [Documentation DataDrivenDiffEq.jl](https://datadriven.sciml.ai/stable), [linear ODE example](https://datadriven.sciml.ai/stable/examples/2_linear_continuous_system/), [nonlinear ODE example](https://datadriven.sciml.ai/stable/examples/4_nonlinear_continuous_system/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.jolin.io\" target=\"_blank\" rel=\"noreferrer noopener\">\n",
    "<img src=\"https://www.jolin.io/assets/Jolin/Jolin-Banner-Website-v1.1-darkmode.webp\">\n",
    "</a>\n",
    "\n",
    "#### Supported by [Jolin.io](https://www.jolin.io)\n",
    "\n",
    "Jolin.io is an IT-consultancy for high-performance computing and data science\n",
    "\n",
    "We are there to help you, if you want to\n",
    "- try out Julia at your company, or\n",
    "- transition Matlab, Fortran, R, Python, etc. to Julia\n",
    "- or speed up your existing code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "520e995520d0f28b9f1e7cacfd9ba1493aa60b57e5f0cc1543205df7dd9220a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

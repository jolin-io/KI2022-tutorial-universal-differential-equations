{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jolin-io/KI2022-tutorial-universal-differential-equations/main?filepath=02%20introduction%20to%20deep%20learning%20in%20julia.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.jolin.io\" target=\"_blank\" rel=\"noreferrer noopener\">\n",
    "<img src=\"https://www.jolin.io/assets/Jolin/Jolin-Banner-Website-v1.1-darkmode.webp\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to deep learning in <img height=\"60px\" style='height:60px;display:inline;' alt=\"Julia\" src=\"https://julialang.org/assets/infra/logo.svg\">\n",
    "\n",
    "There are three well-known deep-learning packages for julia, all written in pure julia:\n",
    "- [Knet.jl](https://github.com/denizyuret/Knet.jl)\n",
    "- [Flux.jl](https://github.com/FluxML/Flux.jl)\n",
    "- [Lux.jl](https://github.com/avik-pal/Lux.jl)\n",
    "\n",
    "We use Lux. It was created as a more functional alternative to Flux.jl in order to address problems with the implicit parameter handling of its interface. One main application of Lux is the combination with Differential Equations, which we will see the next notebook.\n",
    "\n",
    "Our goal of this notebook is to introduce Lux with the example of fitting a polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Lux\n",
    "import NNlib, Plots, Random, Statistics, Zygote, Optimization, OptimizationOptimisers, ComponentArrays\n",
    "\n",
    "rng = Random.default_rng()\n",
    "Random.seed!(rng, 12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's fit a polynomial\n",
    "\n",
    "ðŸ‘‰ Generate 128 datapoints from the polynomial $y = xÂ² - 2x$ and add some noise.\n",
    "\n",
    "You need `randn`, and you might use `range` for x.\n",
    "\n",
    "Plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your space\n",
    "# x = ...\n",
    "# y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lux barebones\n",
    "\n",
    "create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lux.Chain(\n",
    "    Lux.Dense(1, 16, NNlib.relu),\n",
    "    Lux.Dense(16, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps, st = Lux.setup(rng, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, st_updated = Lux.apply(model, x, ps, st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We use Optimization.jl because it is the one meta package which includes unbelievable many optimization routines, including those typical for deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define what we want to optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_function(x, y, ps, st)\n",
    "    y_pred = # ðŸ‘‰ how to get the prediction from our neural network?\n",
    "    sum(abs2, y .- y_pred), st\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function(x, y, ps, st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can combine these with your automatic differentiation package of choice.\n",
    "However, be cautious. Because Lux uses nested NamedTuples for parameters `ps` and states `st`, it might be that your particular autodiff package does not yet support it out-of-the-box.\n",
    "\n",
    "Or just use Optimization.jl as we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 500\n",
    "\n",
    "losses = Float64[]\n",
    "function callback(p, l)\n",
    "    push!(losses, l)\n",
    "    if length(losses) % 50 == 0\n",
    "        Plots.plot(losses, show = :inline, yscale = :log10,\n",
    "            label = \"loss\", xlabel = \"#epochs\", ylabel=\"loss (log10 scale)\")\n",
    "    end\n",
    "    # return bool `halt`\n",
    "    return false\n",
    "end\n",
    "\n",
    "# use Ref to handle updates to Lux state `st`\n",
    "ps_trained, st_trained = let st=Ref(st), x=x, y=y\n",
    "    \n",
    "    opt_prob = Optimization.OptimizationProblem(\n",
    "        Optimization.OptimizationFunction(\n",
    "            function(ps, constants)\n",
    "                loss, st[] = loss_function(x, y, ps, st[])\n",
    "                loss\n",
    "            end,\n",
    "            Optimization.AutoZygote()\n",
    "        ),\n",
    "        ComponentArrays.ComponentVector(ps),\n",
    "    )\n",
    "    \n",
    "    opt_sol = Optimization.solve(\n",
    "        opt_prob,\n",
    "        OptimizationOptimisers.ADAM(0.1),\n",
    "        callback = callback,\n",
    "        maxiters = n_iterations,\n",
    "    )\n",
    "    \n",
    "    opt_sol.minimizer, st[]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred, st_pred = Lux.apply(model, x, ps_trained, st_trained)\n",
    "\n",
    "Plots.plot(x -> evalpoly(x, (0, -2, 1)), x[1, :]; label=false)\n",
    "Plots.scatter!(x[1, :], y[1, :]; label=\"Actual Data\", markersize=3)\n",
    "Plots.scatter!(x[1, :], y_pred[1, :]; label=\"Predictions\", markersize=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ‘‰ adapt the polynomial and fit something different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your space. It is probably easier to just change the above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That was the introduction to deep learning in julia - Thank you for participating ðŸ™‚\n",
    "\n",
    "Next topic is Universal Differential Equations, i.e. one way to combine deep learning methods with differential equations: [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jolin-io/KI2022-tutorial-universal-differential-equations/main?filepath=03%20deep%20dive%20into%20universal%20differential%20equations.ipynb)\n",
    "\n",
    "If you have question, suggestions, or you are just interested in Julia, contact me:\n",
    "- Stephan Sahm stephan.sahm@jolin.io\n",
    "\n",
    "### Further material\n",
    "- [Lux.jl documentation](http://lux.csail.mit.edu/stable/)\n",
    "- especially the [Lux.jl interface](http://lux.csail.mit.edu/stable/manual/interface/)\n",
    "- and the [NeuralODE example](http://lux.csail.mit.edu/stable/examples/generated/intermediate/NeuralODE/main/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.jolin.io\" target=\"_blank\" rel=\"noreferrer noopener\">\n",
    "<img src=\"https://www.jolin.io/assets/Jolin/Jolin-Banner-Website-v1.1-darkmode.webp\">\n",
    "</a>\n",
    "\n",
    "#### Supported by [Jolin.io](https://www.jolin.io)\n",
    "\n",
    "Jolin.io is an IT-consultancy for high-performance computing and data science\n",
    "\n",
    "We are there to help you, if you want to\n",
    "- try out Julia at your company, or\n",
    "- transition Matlab, Fortran, R, Python, etc. to Julia\n",
    "- or speed up your existing code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
